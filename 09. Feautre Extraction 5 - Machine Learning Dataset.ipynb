{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the features from the csv file containing all extracted raw posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script takes the csv file containing all posts with train/test label and creates another CSV with the 'normal' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_csv_filename = 'data/PostsList.csv'\n",
    "raw_nonulls_csv_filename = 'Data/PostList-NoNulls.csv'\n",
    "\n",
    "pos_ngram_list_filename = 'data/POS-ngram-selected.csv'\n",
    "bleached_ngram_list_filename = 'data/Bleached-ngram-selected.csv'\n",
    "\n",
    "pos_csv_filename = 'data/POS.csv'\n",
    "bleached_csv_filename = 'data/Bleached.csv'\n",
    "\n",
    "functionWord_list_filename = 'data/FunctionWords-selected.txt'\n",
    "\n",
    "# output of this notebook\n",
    "feature_csv_filename = 'data/MachineLearningData.csv'\n",
    "\n",
    "punctuation = string.punctuation + ' '\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a preprocessing step, we will remove nulls from the PostLists.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # check if file exists\n",
    "    f_ = open(raw_nonulls_csv_filename)\n",
    "    f_.close()\n",
    "except IOError:\n",
    "    number_of_posts=0\n",
    "    chunksize=10\n",
    "    for chunk in pd.read_csv(raw_csv_filename, usecols=['PostID'], chunksize=chunksize):\n",
    "        number_of_posts = np.max(chunk.PostID)\n",
    "\n",
    "    number_of_chunks = math.ceil(number_of_posts / chunksize)\n",
    "\n",
    "    # Iitialise CSV\n",
    "    df = pd.DataFrame(columns=['PostID', 'UserID', 'Gender', 'Post', 'TrainTest'])\n",
    "    df.to_csv(raw_nonulls_csv_filename, index=False)\n",
    "\n",
    "    # open CSV file in append mode\n",
    "    with open(raw_nonulls_csv_filename, 'a') as f:\n",
    "        for i,chunk in enumerate(pd.read_csv(raw_csv_filename, chunksize=chunksize)):\n",
    "            chunk = chunk.dropna()\n",
    "\n",
    "            if i%100 == 0:\n",
    "                clear_output()\n",
    "                print('{0}/{1}'.format(i+1, number_of_chunks), end = '', flush=True)\n",
    "\n",
    "            chunk.to_csv(f, header=False, index=False)\n",
    "\n",
    "    clear_output()\n",
    "    print('DONE - {0} number_of_posts'.format(number_of_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(string, delimeter_string):\n",
    "    first_delim = delimeter_string[0]\n",
    "    for current_delim in delimeter_string[1:]:\n",
    "        string = string.replace(current_delim, first_delim)\n",
    "    splits = string.split(first_delim)\n",
    "    \n",
    "    ret_splits = []\n",
    "    for string in splits:\n",
    "        if len(string) > 0:\n",
    "            ret_splits.append(string)\n",
    "    \n",
    "    return ret_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the required columns as well get the lists from the data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "['PostID' 'WordCount' 'SentenceCount' 'AvgWordLength' 'AvgSentenceLength'\n",
      " 'UniqueWordsPercentage' 'URLCount' 'ableWords' 'alWords' 'fulWords'\n",
      " 'ibleWords' 'icWords' 'iveWords' 'lessWords' 'lyWords' 'ousWords'\n",
      " 'Gender' 'TrainTest' 'pos_VB_VBP' 'pos_._.' 'pos_NNP_NNPS' 'pos_JJ_VBP'\n",
      " 'pos_NNP_``' 'pos_NNP_TO' 'pos_DT_NNP' 'pos_NNP_MD' 'pos_NNP_)' 'pos_._)'\n",
      " 'pos_CD_NNP' 'pos_._NN' 'pos_NNP_NNS' 'pos_)_PRP' 'pos_NN_VBP'\n",
      " 'pos_PRP_CC' 'pos_(_NNP' 'pos_``_DT' 'pos_NNP_(' 'pos_PRP_WRB'\n",
      " 'pos_NNP_VBZ' 'pos_POS_NNP' 'pos_(_CD' 'pos_``_NNP' 'pos_IN_NNP'\n",
      " 'pos_NNS_VBZ' 'pos_NNP_CD' 'pos_._VBN' 'pos_JJ_PRP' 'pos_VBG_NNP'\n",
      " 'pos_CD_)' 'pos_CD_,' 'pos_VBP_VBP' 'pos_NNP_DT' 'pos_VBN_VBN' 'pos_._('\n",
      " 'pos_NNP_IN' 'pos_NNP_:' 'pos_CD_:' 'pos_:_NNP' 'pos_VBP_PRP'\n",
      " 'pos_VBP_WP' 'pos_PRP_PRP' 'pos_PRP_NN' 'pos_VBP_WRB' 'pos_PRP_.'\n",
      " 'pos_WP_NN' 'pos_DT_``' 'pos_PRP_VB' 'pos_VBN_DT' 'bleached_CVV_?'\n",
      " 'bleached_VCCCVCC_CVCCVC' 'bleached_--_--' 'bleached_VCCCVCC_CVCCVCC'\n",
      " 'bleached_!_V' 'bleached_VCCCVCC_CCCC' 'bleached_VCV_CVV'\n",
      " 'bleached_CVV_!' 'bleached_CCCC_:' 'bleached_!_CV' 'bleached_!_!'\n",
      " 'bleached_!_CVC' 'bleached_._:' 'bleached_CVCCC_!' 'bleached_!_)'\n",
      " 'bleached_!_VCC' 'bleached_?_?' 'bleached_!_VC' 'bleached_CV_!'\n",
      " 'bleached_!_CVCV' 'bleached_CVCV_VCCCVCC' 'bleached_CC_VCCCVCC'\n",
      " 'bleached_CVC_!' 'bleached_CCV_VCCCVCC' 'bleached_VC_VCCCVCC'\n",
      " 'bleached_?_!' 'bleached_VC_!' 'bleached_CVCC_!' 'bleached_CCVC_VCCCVCC'\n",
      " 'bleached_VCCCVCC_CVCVC' 'bleached_!_VCCCVCC' 'bleached_CC_CVVCC'\n",
      " 'bleached_CCV_``' \"bleached_CCV_'\" 'bleached_CVCVC_!' 'bleached_!_CCVC'\n",
      " 'bleached_CVCV_!' 'bleached_CVCVVC_VC' 'bleached_!_CVCC'\n",
      " 'bleached_VCCCVCC_CVCC' 'bleached_CVCVVCV_V' 'bleached_:_V'\n",
      " 'bleached_:_)' 'bleached_:_``' 'bleached_CVVC_!' 'bleached_V_VC'\n",
      " 'bleached_CCV_CVCCVCV' 'bleached_VCCCVCC_CVVC' \"bleached_CV_'\"\n",
      " 'bleached_CCV_CVCVCVC' 'fw_him' 'fw_against' 'fw_i' 'fw_she' 'fw_alone'\n",
      " 'fw_her' 'fw_myself' 'fw_sometimes' 'fw_me' 'fw_am' 'fw_etc' 'fw_so'\n",
      " 'fw_my' 'fw_has' 'fw_which' 'fw_he' 'fw_yes' 'fw_always' 'fw_everything'\n",
      " 'fw_their' 'fw_may' 'fw_someone' 'fw_never' 'fw_rather' 'fw_between'\n",
      " 'fw_together' 'fw_also' 'fw_too' 'fw_near' 'fw_inside' 'fw_few' 'fw_as'\n",
      " 'fw_when' 'fw_during' 'fw_often' 'fw_an' 'fw_its' 'fw_did' 'fw_because'\n",
      " 'fw_some' 'fw_by' 'fw_these' 'fw_second' 'fw_do' 'fw_shall' 'fw_of'\n",
      " 'fw_most' 'fw_mine' 'fw_under' 'fw_even']\n"
     ]
    }
   ],
   "source": [
    "other_columns = [\n",
    "    #ID to identify the post\n",
    "    'PostID',\n",
    "    \n",
    "    # Basic Features\n",
    "    'WordCount',\n",
    "    'SentenceCount',\n",
    "    'AvgWordLength',\n",
    "    'AvgSentenceLength',\n",
    "    'UniqueWordsPercentage',\n",
    "    'URLCount',\n",
    "    \n",
    "    # Words ending with some suffix\n",
    "    'ableWords',\n",
    "    'alWords',\n",
    "    'fulWords',\n",
    "    'ibleWords',\n",
    "    'icWords',\n",
    "    'iveWords',\n",
    "    'lessWords',\n",
    "    'lyWords',\n",
    "    'ousWords',\n",
    "    \n",
    "    # Target value\n",
    "    'Gender',\n",
    "    \n",
    "    # Whether it is part of the train or test set\n",
    "    'TrainTest'\n",
    "]\n",
    "\n",
    "# get pos bi-gram feature list\n",
    "pos_dict = {} # dictionary mapping pos to column name\n",
    "df = pd.read_csv(pos_ngram_list_filename)\n",
    "for row in df.iterrows():\n",
    "    pos_dict[ (row[1][0], row[1][1]) ] = 'pos_{0}_{1}'.format(row[1][0], row[1][1])\n",
    "pos_columns = list(pos_dict.values())\n",
    "\n",
    "# get bleached bi-gram feature list\n",
    "bleached_dict = {} # dictionary mapping pos to column name\n",
    "df = pd.read_csv(bleached_ngram_list_filename)\n",
    "for row in df.iterrows():\n",
    "    bleached_dict[ (row[1][0], row[1][1]) ] = 'bleached_{0}_{1}'.format(row[1][0], row[1][1])\n",
    "bleached_columns = list(bleached_dict.values())\n",
    "\n",
    "# get function word feature list\n",
    "fw_dict = {}\n",
    "with open (functionWord_list_filename, 'r') as f:\n",
    "    words = f.readlines()\n",
    "for word in words:\n",
    "    word = word[:-1] # remove '\\n' at the end\n",
    "    fw_dict[word] = 'fw_{0}'.format(word)\n",
    "fw_columns = list(fw_dict.values())\n",
    "\n",
    "df = pd.DataFrame(columns=other_columns + pos_columns + bleached_columns + fw_columns)\n",
    "df.to_csv(feature_csv_filename, index=False)\n",
    "\n",
    "print(len(df.columns.values))\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main functions to parse a post and return the entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_raw(post):\n",
    "    words = split(post, punctuation)\n",
    "    sentences = split(post, '.?!')\n",
    "\n",
    "    entry = {\n",
    "        'WordCount': getWordCount(words),\n",
    "        'SentenceCount': getSentenceCount(sentences),\n",
    "        'AvgWordLength': getAvgWordLength(words),\n",
    "        'AvgSentenceLength': getAvgSentenceLength(sentences),\n",
    "        'UniqueWordsPercentage': UniqueWordsPercentage(words),\n",
    "        'URLCount': getURLCount(words),\n",
    "    }\n",
    "    entry.update( getWordsEndingDict(words) )\n",
    "\n",
    "    entry.update( getFunctionWordFrequencies(post) )\n",
    "    \n",
    "    return entry\n",
    "\n",
    "def extract_data_pos(post):\n",
    "    entry = dict.fromkeys( pos_columns , 0)\n",
    "    \n",
    "    if len(list(nltk.bigrams(nltk.word_tokenize(post)))) > 0:\n",
    "        inverse_number_of_bigrams = 1/len(list(nltk.bigrams(nltk.word_tokenize(post))))\n",
    "        \n",
    "        bigrams = nltk.bigrams(nltk.word_tokenize(post))\n",
    "        for bigram in bigrams:\n",
    "            if bigram in pos_dict:\n",
    "                entry[ pos_dict[bigram] ] += inverse_number_of_bigrams  \n",
    "    return entry\n",
    "\n",
    "def extract_data_bleached(post):\n",
    "    entry = dict.fromkeys( bleached_columns , 0)\n",
    "    \n",
    "    if len(list(nltk.bigrams(nltk.word_tokenize(post)))) > 0:\n",
    "        inverse_number_of_bigrams = 1/len(list(nltk.bigrams(nltk.word_tokenize(post))))\n",
    "        \n",
    "        bigrams = nltk.bigrams(nltk.word_tokenize(post))\n",
    "        for bigram in bigrams:\n",
    "            if bigram in bleached_dict:\n",
    "                entry[ bleached_dict[bigram] ] += inverse_number_of_bigrams\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to parse blog posts individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCount(words):\n",
    "    return len(words)\n",
    "\n",
    "def getSentenceCount(sentences):\n",
    "    return len(sentences)\n",
    "\n",
    "def getAvgWordLength(words):\n",
    "    return np.mean( np.asarray(list( map(len, words) )) )\n",
    "\n",
    "def getAvgSentenceLength(sentences):\n",
    "    sentences = [split(s,punctuation) for s in sentences]\n",
    "    return np.mean( np.asarray(list( map(len, sentences) )) )\n",
    "\n",
    "def UniqueWordsPercentage(words):\n",
    "    if len(words) == 0: return 0\n",
    "    return len( np.unique(words) ) / len(words)\n",
    "\n",
    "def getURLCount(words):\n",
    "    return words.count('urlLink')/len(words)\n",
    "\n",
    "def getWordsEndingDict(words):\n",
    "    no_of_words = len(words)\n",
    "    return {\n",
    "        'ableWords': get_no_of_words_ending_with('able', words)/no_of_words,\n",
    "        'alWords': get_no_of_words_ending_with('al', words)/no_of_words,\n",
    "        'fulWords': get_no_of_words_ending_with('ful', words)/no_of_words,\n",
    "        'ibleWords': get_no_of_words_ending_with('ible', words)/no_of_words,\n",
    "        'icWords': get_no_of_words_ending_with('ic', words)/no_of_words,\n",
    "        'iveWords': get_no_of_words_ending_with('ive', words)/no_of_words,\n",
    "        'lessWords': get_no_of_words_ending_with('less', words)/no_of_words,\n",
    "        'lyWords': get_no_of_words_ending_with('ly', words)/no_of_words,\n",
    "        'ousWords': get_no_of_words_ending_with('ous', words)/no_of_words\n",
    "    }\n",
    "\n",
    "def get_no_of_words_ending_with(suffix, words):\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word.endswith(suffix):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def getFunctionWordFrequencies(post):\n",
    "    entry = dict.fromkeys( fw_columns , 0)\n",
    "    \n",
    "    words = nltk.word_tokenize(post)\n",
    "    if len(words) > 0:\n",
    "        inverse_number_of_words = 1/len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            if word in entry:\n",
    "                entry[word] += inverse_number_of_words\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_posts=0\n",
    "chunksize=10\n",
    "for chunk in pd.read_csv(raw_nonulls_csv_filename, usecols=['PostID'], chunksize=chunksize):\n",
    "    number_of_posts = np.max(chunk.PostID)\n",
    "\n",
    "number_of_chunks = math.ceil(number_of_posts / chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open CSV file in append mode\n",
    "with open(feature_csv_filename, 'a') as f:\n",
    "    for i, ( raw_chunk, pos_chunk, bleached_chunk) in enumerate(zip(\n",
    "        pd.read_csv(raw_nonulls_csv_filename,chunksize=chunksize),\n",
    "        pd.read_csv(pos_csv_filename, chunksize=chunksize),\n",
    "        pd.read_csv(bleached_csv_filename, chunksize=chunksize)\n",
    "    )):\n",
    "        \n",
    "        clear_output()\n",
    "        print('{0}/{1}'.format(i+1, number_of_chunks), end = '', flush=True)\n",
    "        \n",
    "        # Extract data from each file\n",
    "        df = df.iloc[0:0]\n",
    "        for (raw_row, pos_row, bleached_row) in zip(\n",
    "            raw_chunk.iterrows(),\n",
    "            pos_chunk.iterrows(),\n",
    "            bleached_chunk.iterrows()\n",
    "        ):\n",
    "            if raw_row[1][0] != pos_row[1][0] or raw_row[1][0] != bleached_row[1][0]:\n",
    "                raise Exception('Error in data, PostIDs are not equal at row {0}'.format(i))\n",
    "                \n",
    "            entry = {\n",
    "                'PostID': raw_row[1][0],\n",
    "                'Gender': raw_row[1][2],\n",
    "                'TrainTest': raw_row[1][4]\n",
    "            }\n",
    "            entry.update( extract_data_raw(raw_row[1][3]) )\n",
    "            entry.update( extract_data_pos(pos_row[1][1]) )\n",
    "            entry.update( extract_data_bleached(bleached_row[1][1]) )\n",
    "            \n",
    "            df = df.append(entry, ignore_index=True)\n",
    "\n",
    "        df.to_csv(f, header=False, index=False)\n",
    "        \n",
    "clear_output()\n",
    "print('DONE - {0} number_of_posts'.format(number_of_posts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
