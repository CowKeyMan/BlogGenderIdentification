{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Machine Learning Model and Evalution 2 - Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script trains a baseline and 'main' model on the scaled training data and evaluates them on the test set after scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import reciprocal, uniform\n",
    "from joblib import dump, load\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = 'data/MachineLearningData.csv'\n",
    "\n",
    "baseline_model_filename = 'TrainedModels/Baseline.bin'\n",
    "final_model_filename = 'TrainedModels/Final.bin'\n",
    "\n",
    "svm_model_filename = 'TrainedModels/SVM.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(targets = [], predicted = [], labels = []):\n",
    "    cm = metrics.confusion_matrix(targets, predicted)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.rcParams[\"figure.figsize\"] =  [15, 9]\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostID</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>SentenceCount</th>\n",
       "      <th>AvgWordLength</th>\n",
       "      <th>AvgSentenceLength</th>\n",
       "      <th>UniqueWordsPercentage</th>\n",
       "      <th>URLCount</th>\n",
       "      <th>ableWords</th>\n",
       "      <th>alWords</th>\n",
       "      <th>fulWords</th>\n",
       "      <th>...</th>\n",
       "      <th>fw_by</th>\n",
       "      <th>fw_these</th>\n",
       "      <th>fw_second</th>\n",
       "      <th>fw_do</th>\n",
       "      <th>fw_shall</th>\n",
       "      <th>fw_of</th>\n",
       "      <th>fw_most</th>\n",
       "      <th>fw_mine</th>\n",
       "      <th>fw_under</th>\n",
       "      <th>fw_even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>277565.0</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>3.783333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>573553.0</td>\n",
       "      <td>246</td>\n",
       "      <td>13</td>\n",
       "      <td>4.130081</td>\n",
       "      <td>18.923077</td>\n",
       "      <td>0.617886</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>210974.0</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>3.567164</td>\n",
       "      <td>9.571429</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>424247.0</td>\n",
       "      <td>669</td>\n",
       "      <td>59</td>\n",
       "      <td>4.240658</td>\n",
       "      <td>11.338983</td>\n",
       "      <td>0.562033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>294114.0</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676308</td>\n",
       "      <td>501847.0</td>\n",
       "      <td>307</td>\n",
       "      <td>39</td>\n",
       "      <td>3.469055</td>\n",
       "      <td>7.871795</td>\n",
       "      <td>0.602606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676309</td>\n",
       "      <td>485774.0</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>3.591837</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676310</td>\n",
       "      <td>203910.0</td>\n",
       "      <td>110</td>\n",
       "      <td>11</td>\n",
       "      <td>4.081818</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676311</td>\n",
       "      <td>169603.0</td>\n",
       "      <td>280</td>\n",
       "      <td>17</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.470588</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676312</td>\n",
       "      <td>406609.0</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>3.490909</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676313 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PostID  WordCount  SentenceCount  AvgWordLength  AvgSentenceLength  \\\n",
       "0       277565.0        120             12       3.783333          10.000000   \n",
       "1       573553.0        246             13       4.130081          18.923077   \n",
       "2       210974.0         67              7       3.567164           9.571429   \n",
       "3       424247.0        669             59       4.240658          11.338983   \n",
       "4       294114.0         35              4       4.857143           8.750000   \n",
       "...          ...        ...            ...            ...                ...   \n",
       "676308  501847.0        307             39       3.469055           7.871795   \n",
       "676309  485774.0         49              5       3.591837           9.800000   \n",
       "676310  203910.0        110             11       4.081818          10.000000   \n",
       "676311  169603.0        280             17       5.000000          16.470588   \n",
       "676312  406609.0         55              4       3.490909          13.750000   \n",
       "\n",
       "        UniqueWordsPercentage  URLCount  ableWords   alWords  fulWords  ...  \\\n",
       "0                    0.708333  0.000000   0.008333  0.000000  0.000000  ...   \n",
       "1                    0.617886  0.016260   0.000000  0.008130  0.000000  ...   \n",
       "2                    0.791045  0.000000   0.000000  0.000000  0.000000  ...   \n",
       "3                    0.562033  0.000000   0.001495  0.010463  0.000000  ...   \n",
       "4                    0.942857  0.000000   0.000000  0.028571  0.000000  ...   \n",
       "...                       ...       ...        ...       ...       ...  ...   \n",
       "676308               0.602606  0.000000   0.000000  0.000000  0.003257  ...   \n",
       "676309               0.857143  0.000000   0.000000  0.000000  0.000000  ...   \n",
       "676310               0.809091  0.000000   0.000000  0.009091  0.009091  ...   \n",
       "676311               0.696429  0.007143   0.007143  0.010714  0.000000  ...   \n",
       "676312               0.709091  0.000000   0.000000  0.000000  0.000000  ...   \n",
       "\n",
       "           fw_by  fw_these  fw_second     fw_do  fw_shall     fw_of   fw_most  \\\n",
       "0       0.000000  0.000000        0.0  0.000000       0.0  0.013605  0.000000   \n",
       "1       0.003774  0.000000        0.0  0.000000       0.0  0.018868  0.000000   \n",
       "2       0.000000  0.000000        0.0  0.000000       0.0  0.013699  0.000000   \n",
       "3       0.001323  0.000000        0.0  0.001323       0.0  0.021164  0.001323   \n",
       "4       0.000000  0.000000        0.0  0.000000       0.0  0.024390  0.000000   \n",
       "...          ...       ...        ...       ...       ...       ...       ...   \n",
       "676308  0.000000  0.000000        0.0  0.000000       0.0  0.006042  0.003021   \n",
       "676309  0.000000  0.018182        0.0  0.000000       0.0  0.018182  0.000000   \n",
       "676310  0.000000  0.007634        0.0  0.007634       0.0  0.015267  0.000000   \n",
       "676311  0.000000  0.000000        0.0  0.003247       0.0  0.025974  0.000000   \n",
       "676312  0.000000  0.000000        0.0  0.000000       0.0  0.000000  0.000000   \n",
       "\n",
       "       fw_mine  fw_under   fw_even  \n",
       "0          0.0       0.0  0.006803  \n",
       "1          0.0       0.0  0.000000  \n",
       "2          0.0       0.0  0.000000  \n",
       "3          0.0       0.0  0.000000  \n",
       "4          0.0       0.0  0.000000  \n",
       "...        ...       ...       ...  \n",
       "676308     0.0       0.0  0.003021  \n",
       "676309     0.0       0.0  0.000000  \n",
       "676310     0.0       0.0  0.000000  \n",
       "676311     0.0       0.0  0.000000  \n",
       "676312     0.0       0.0  0.000000  \n",
       "\n",
       "[676313 rows x 168 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: We call this df_train for memory issues\n",
    "df_train = pd.read_csv(data_filename).dropna()\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = df_train[ df_train['TrainTest'] == 'test' ]\n",
    "\n",
    "# df_male = df_test[ df_test['Gender'] == 'male' ][:5000]\n",
    "# df_female = df_test[ df_test['Gender'] == 'female' ][:5000]\n",
    "# df_test = df_male.append(df_female, ignore_index=True)\n",
    "\n",
    "y_test = df_test['Gender']\n",
    "df_test = df_test.drop(['TrainTest', 'Gender', 'PostID'], axis=1)\n",
    "\n",
    "df_train = df_train[ df_train['TrainTest'] == 'train' ]\n",
    "\n",
    "# df_male = df_train[ df_train['Gender'] == 'male' ][:5000]\n",
    "# df_female = df_train[ df_train['Gender'] == 'female' ][:5000]\n",
    "# df_train = df_male.append(df_female, ignore_index=True)\n",
    "\n",
    "y_train = df_train['Gender']\n",
    "df_train = df_train.drop(['TrainTest', 'Gender', 'PostID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many train and test items there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TOTAL training examples: 537215\n",
      "Number of MALE training examples: 268572\n",
      "Number of FEMALE training examples: 268643\n",
      "\n",
      "Number of TOTAL testing examples: 139098\n",
      "Number of MALE testing examples: 74300\n",
      "Number of FEMALE testing examples: 64798\n"
     ]
    }
   ],
   "source": [
    "print('Number of TOTAL training examples: {0}'.format(df_train.shape[0]))\n",
    "print('Number of MALE training examples: {0}'.format(np.sum(y_train == 'male' )))\n",
    "print('Number of FEMALE training examples: {0}'.format(np.sum(y_train == 'female')))\n",
    "print()\n",
    "print('Number of TOTAL testing examples: {0}'.format(df_test.shape[0]))\n",
    "print('Number of MALE testing examples: {0}'.format(np.sum(y_test == 'male')))\n",
    "print('Number of FEMALE testing examples: {0}'.format(np.sum(y_test == 'female')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the scaler on the train set and transform the train set to scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.193335</td>\n",
       "      <td>-0.130153</td>\n",
       "      <td>-0.030846</td>\n",
       "      <td>-0.205973</td>\n",
       "      <td>-0.076758</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>1.519484</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.049277</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>1.902201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.301589</td>\n",
       "      <td>-0.254424</td>\n",
       "      <td>-0.047844</td>\n",
       "      <td>-0.228945</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.928015</td>\n",
       "      <td>1.037994</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.134204</td>\n",
       "      <td>-0.963664</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>0.090949</td>\n",
       "      <td>0.421532</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094122</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.293930</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.545057</td>\n",
       "      <td>0.296444</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.366950</td>\n",
       "      <td>-0.328987</td>\n",
       "      <td>0.053589</td>\n",
       "      <td>-0.272973</td>\n",
       "      <td>1.344978</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>1.855267</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.756670</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.439850</td>\n",
       "      <td>0.317222</td>\n",
       "      <td>0.019655</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>-0.564320</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>0.145472</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>1.116647</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.206888</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.656891</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>0.389854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537210</td>\n",
       "      <td>0.188619</td>\n",
       "      <td>0.540910</td>\n",
       "      <td>-0.055558</td>\n",
       "      <td>-0.320045</td>\n",
       "      <td>-0.717701</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>0.429406</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>-0.446802</td>\n",
       "      <td>0.944878</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>0.691492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537211</td>\n",
       "      <td>-0.338355</td>\n",
       "      <td>-0.304133</td>\n",
       "      <td>-0.045903</td>\n",
       "      <td>-0.216693</td>\n",
       "      <td>0.825359</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>5.461440</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.349449</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537212</td>\n",
       "      <td>-0.213760</td>\n",
       "      <td>-0.155008</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-0.205973</td>\n",
       "      <td>0.534057</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>0.312864</td>\n",
       "      <td>1.482262</td>\n",
       "      <td>2.641672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>2.193436</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537213</td>\n",
       "      <td>0.133470</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.064822</td>\n",
       "      <td>0.140850</td>\n",
       "      <td>-0.148927</td>\n",
       "      <td>-0.172357</td>\n",
       "      <td>1.270801</td>\n",
       "      <td>0.441398</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.072482</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.860553</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537214</td>\n",
       "      <td>-0.326099</td>\n",
       "      <td>-0.328987</td>\n",
       "      <td>-0.053840</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>-0.072165</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>-0.843127</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537215 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.193335 -0.130153 -0.030846 -0.205973 -0.076758 -0.256883  1.519484   \n",
       "1      -0.301589 -0.254424 -0.047844 -0.228945  0.424658 -0.256883 -0.221299   \n",
       "2       0.928015  1.037994  0.005114 -0.134204 -0.963664 -0.256883  0.090949   \n",
       "3      -0.366950 -0.328987  0.053589 -0.272973  1.344978 -0.256883 -0.221299   \n",
       "4       0.439850  0.317222  0.019655  0.026293 -0.564320 -0.256883 -0.221299   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "537210  0.188619  0.540910 -0.055558 -0.320045 -0.717701 -0.256883 -0.221299   \n",
       "537211 -0.338355 -0.304133 -0.045903 -0.216693  0.825359 -0.256883 -0.221299   \n",
       "537212 -0.213760 -0.155008 -0.007376 -0.205973  0.534057 -0.256883 -0.221299   \n",
       "537213  0.133470 -0.005882  0.064822  0.140850 -0.148927 -0.172357  1.270801   \n",
       "537214 -0.326099 -0.328987 -0.053840 -0.004974 -0.072165 -0.256883 -0.221299   \n",
       "\n",
       "             7         8         9    ...       155       156       157  \\\n",
       "0      -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "1      -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "2       0.421532 -0.158484 -0.141943  ... -0.094122 -0.171567 -0.109221   \n",
       "3       1.855267 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "4       0.145472 -0.158484 -0.141943  ...  0.006529  1.116647 -0.109221   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "537210 -0.406924  0.429406 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "537211 -0.406924 -0.158484 -0.141943  ... -0.270169  5.461440 -0.109221   \n",
       "537212  0.312864  1.482262  2.641672  ... -0.270169  2.193436 -0.109221   \n",
       "537213  0.441398 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "537214 -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "\n",
       "             158       159       160       161       162       163       164  \n",
       "0      -0.446176 -0.074722  0.049277 -0.208572 -0.075663 -0.085034  1.902201  \n",
       "1      -0.446176 -0.074722  0.055389 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "2      -0.293930 -0.074722  0.545057  0.296444 -0.075663 -0.085034 -0.275759  \n",
       "3      -0.446176 -0.074722  0.756670 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "4      -0.206888 -0.074722  0.656891 -0.208572 -0.075663 -0.085034  0.389854  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "537210 -0.446176 -0.074722 -0.446802  0.944878 -0.075663 -0.085034  0.691492  \n",
       "537211 -0.446176 -0.074722  0.349449 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "537212  0.432432 -0.074722  0.158273 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "537213 -0.072482 -0.074722  0.860553 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "537214 -0.446176 -0.074722 -0.843127 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "\n",
       "[537215 rows x 165 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n",
    "\n",
    "df_train = pd.DataFrame(scaler.fit_transform(df_train.values))\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the test set with the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.064024</td>\n",
       "      <td>-0.105299</td>\n",
       "      <td>-0.003581</td>\n",
       "      <td>0.272303</td>\n",
       "      <td>-0.625069</td>\n",
       "      <td>-0.064467</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>0.236789</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232064</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.394452</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.389418</td>\n",
       "      <td>-0.353841</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>-0.313173</td>\n",
       "      <td>0.933613</td>\n",
       "      <td>0.236182</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>-0.843127</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.034017</td>\n",
       "      <td>-0.080445</td>\n",
       "      <td>-0.016589</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>-0.604906</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.031429</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>1.147175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.736017</td>\n",
       "      <td>0.491202</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.090999</td>\n",
       "      <td>-1.376622</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.269225</td>\n",
       "      <td>0.155398</td>\n",
       "      <td>0.390575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152345</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.263481</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>0.406239</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.279121</td>\n",
       "      <td>-0.353841</td>\n",
       "      <td>-0.008769</td>\n",
       "      <td>0.651625</td>\n",
       "      <td>0.681020</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>-0.097767</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139093</td>\n",
       "      <td>0.448020</td>\n",
       "      <td>0.292368</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>0.060178</td>\n",
       "      <td>-0.822896</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.042054</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>1.159846</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>1.177151</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139094</td>\n",
       "      <td>-0.420056</td>\n",
       "      <td>-0.403550</td>\n",
       "      <td>0.091032</td>\n",
       "      <td>-0.259573</td>\n",
       "      <td>1.017811</td>\n",
       "      <td>1.057957</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>8.390488</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>5.119753</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139095</td>\n",
       "      <td>-0.395546</td>\n",
       "      <td>-0.378695</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.179173</td>\n",
       "      <td>0.825359</td>\n",
       "      <td>0.306620</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>-0.843127</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139096</td>\n",
       "      <td>0.184534</td>\n",
       "      <td>0.342076</td>\n",
       "      <td>-0.032113</td>\n",
       "      <td>-0.214618</td>\n",
       "      <td>-0.673870</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>0.371863</td>\n",
       "      <td>0.433261</td>\n",
       "      <td>0.861984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117854</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.110614</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>-0.269439</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>0.657652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139097</td>\n",
       "      <td>-0.389418</td>\n",
       "      <td>-0.378695</td>\n",
       "      <td>-0.020362</td>\n",
       "      <td>-0.098773</td>\n",
       "      <td>1.438798</td>\n",
       "      <td>-0.256883</td>\n",
       "      <td>-0.221299</td>\n",
       "      <td>-0.406924</td>\n",
       "      <td>-0.158484</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270169</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.446176</td>\n",
       "      <td>-0.074722</td>\n",
       "      <td>1.499433</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.075663</td>\n",
       "      <td>-0.085034</td>\n",
       "      <td>-0.275759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139098 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0       0.064024 -0.105299 -0.003581  0.272303 -0.625069 -0.064467 -0.221299   \n",
       "1      -0.389418 -0.353841  0.002572 -0.313173  0.933613  0.236182 -0.221299   \n",
       "2      -0.034017 -0.080445 -0.016589  0.016084 -0.604906 -0.256883 -0.221299   \n",
       "3       0.736017  0.491202  0.002464  0.090999 -1.376622 -0.256883 -0.221299   \n",
       "4      -0.279121 -0.353841 -0.008769  0.651625  0.681020 -0.256883 -0.221299   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "139093  0.448020  0.292368  0.017716  0.060178 -0.822896 -0.256883 -0.221299   \n",
       "139094 -0.420056 -0.403550  0.091032 -0.259573  1.017811  1.057957 -0.221299   \n",
       "139095 -0.395546 -0.378695  0.001168 -0.179173  0.825359  0.306620 -0.221299   \n",
       "139096  0.184534  0.342076 -0.032113 -0.214618 -0.673870 -0.256883 -0.221299   \n",
       "139097 -0.389418 -0.378695 -0.020362 -0.098773  1.438798 -0.256883 -0.221299   \n",
       "\n",
       "             7         8         9    ...       155       156       157  \\\n",
       "0       0.236789 -0.158484 -0.141943  ...  0.232064 -0.171567 -0.109221   \n",
       "1      -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "2      -0.007042 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "3      -0.269225  0.155398  0.390575  ...  0.152345 -0.171567 -0.109221   \n",
       "4      -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "139093 -0.042054 -0.158484 -0.141943  ...  0.003120 -0.171567  1.159846   \n",
       "139094  8.390488 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "139095 -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "139096  0.371863  0.433261  0.861984  ...  0.117854 -0.171567 -0.109221   \n",
       "139097 -0.406924 -0.158484 -0.141943  ... -0.270169 -0.171567 -0.109221   \n",
       "\n",
       "             158       159       160       161       162       163       164  \n",
       "0      -0.446176 -0.074722  0.394452 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "1      -0.446176 -0.074722 -0.843127 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "2      -0.446176 -0.074722  0.031429 -0.208572 -0.075663 -0.085034  1.147175  \n",
       "3      -0.263481 -0.074722  0.406239 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "4      -0.446176 -0.074722 -0.097767 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "139093 -0.446176 -0.074722  1.177151 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "139094 -0.446176 -0.074722  5.119753 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "139095 -0.446176 -0.074722 -0.843127 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "139096 -0.110614 -0.074722 -0.269439 -0.208572 -0.075663 -0.085034  0.657652  \n",
       "139097 -0.446176 -0.074722  1.499433 -0.208572 -0.075663 -0.085034 -0.275759  \n",
       "\n",
       "[139098 rows x 165 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(scaler.transform(df_test.values))\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.561186404424365, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                   random_state=42, solver='newton-cg', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune hyperparmeters by sklearn's random search\n",
    "param_distributions =  {\"C\": uniform(0.001, 10)}\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', tol=0.0001, C=1.0,\n",
    "                        fit_intercept=True, class_weight=None,\n",
    "                        random_state=42, solver='newton-cg', max_iter=100, multi_class='auto',\n",
    "                        verbose=1, warm_start=False, n_jobs=-1, l1_ratio=None)\n",
    "\n",
    "# random search trough the hyperparameters\n",
    "clf_search_cv = RandomizedSearchCV(lr,\n",
    "                                    param_distributions,\n",
    "                                    iid=False,\n",
    "                                    n_iter=20,\n",
    "                                    n_jobs=2,\n",
    "                                    pre_dispatch='2*n_jobs',\n",
    "                                    cv=5,\n",
    "                                    refit=True,\n",
    "                                    verbose=False, \n",
    "                                    random_state=42,\n",
    "                                    return_train_score=False\n",
    "                                   )\n",
    "\n",
    "clf_search_cv.fit(df_train.iloc[:10000], y_train[:10000])\n",
    "\n",
    "lr = clf_search_cv.best_estimator_\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TrainedModels/Baseline.bin']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it\n",
    "lr.fit(df_train, y_train)\n",
    "dump(lr, baseline_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.58      0.57      0.57     64798\n",
      "        male       0.63      0.63      0.63     74300\n",
      "\n",
      "    accuracy                           0.60    139098\n",
      "   macro avg       0.60      0.60      0.60    139098\n",
      "weighted avg       0.60      0.60      0.60    139098\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36734 28064]\n",
      " [27123 47177]]\n"
     ]
    }
   ],
   "source": [
    "lr = load(baseline_model_filename)\n",
    "predictions = lr.predict(df_test)\n",
    "print(metrics.classification_report(y_test, predictions))\n",
    "plot_confusion_matrix(targets=y_test, predicted=predictions, labels=['male', 'female'])\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   42.1s finished\n",
      "/home/danielcauchi/miniconda3/envs/nlp/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:350: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.043204501864211584, batch_size=10000,\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(265,), learning_rate='constant',\n",
       "              learning_rate_init=0.029222914019804192, max_iter=100,\n",
       "              momentum=0.8, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=42, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.2, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune hyperparmeters by sklearn's random search\n",
    "param_distributions =  {'learning_rate_init': uniform(0.0001, 0.1),\n",
    "                       'alpha': uniform(0.00001, 0.1)}\n",
    "\n",
    "mlp = MLPClassifier (hidden_layer_sizes=(df_train.shape[1]+100, ),\n",
    "                     activation='relu',\n",
    "                     solver='adam', alpha=0.0001, batch_size=10000,\n",
    "                     learning_rate='constant', learning_rate_init=0.001,\n",
    "                     max_iter=100, shuffle=True, random_state=42,\n",
    "                     tol=0.0001, verbose=False, warm_start=False, momentum=0.8,\n",
    "                     early_stopping=True, validation_fraction=0.2,\n",
    "                     beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
    "# random search trough the hyperparameters\n",
    "clf_search_cv = RandomizedSearchCV(mlp,\n",
    "                                    param_distributions,\n",
    "                                    iid=False,\n",
    "                                    n_iter=30,\n",
    "                                    n_jobs=-1,\n",
    "                                    pre_dispatch='2*n_jobs',\n",
    "                                    cv=3,\n",
    "                                    refit=True,\n",
    "                                    verbose=True, \n",
    "                                    random_state=42,\n",
    "                                    return_train_score=False\n",
    "                                   )\n",
    "\n",
    "clf_search_cv.fit(df_train.iloc[:1000],\n",
    "                  y_train[:1000])\n",
    "\n",
    "mlp = clf_search_cv.best_estimator_\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Validation score: 0.494532\n",
      "Iteration 2, loss = 0.71295066\n",
      "Validation score: 0.580457\n",
      "Iteration 3, loss = 0.68549491\n",
      "Validation score: 0.603101\n",
      "Iteration 4, loss = inf\n",
      "Validation score: 0.614056\n",
      "Iteration 5, loss = 0.65852487\n",
      "Validation score: 0.613590\n",
      "Iteration 6, loss = 0.65332120\n",
      "Validation score: 0.616801\n",
      "Iteration 7, loss = inf\n",
      "Validation score: 0.618430\n",
      "Iteration 8, loss = 0.64759516\n",
      "Validation score: 0.618607\n",
      "Iteration 9, loss = 0.64581653\n",
      "Validation score: 0.620729\n",
      "Iteration 10, loss = 0.64393314\n",
      "Validation score: 0.620133\n",
      "Iteration 11, loss = 0.64325372\n",
      "Validation score: 0.620161\n",
      "Iteration 12, loss = 0.64227336\n",
      "Validation score: 0.623037\n",
      "Iteration 13, loss = 0.63977132\n",
      "Validation score: 0.623894\n",
      "Iteration 14, loss = 0.64005045\n",
      "Validation score: 0.624238\n",
      "Iteration 15, loss = 0.63772428\n",
      "Validation score: 0.624396\n",
      "Iteration 16, loss = 0.63817028\n",
      "Validation score: 0.621343\n",
      "Iteration 17, loss = 0.63636938\n",
      "Validation score: 0.620757\n",
      "Iteration 18, loss = 0.63601326\n",
      "Validation score: 0.618393\n",
      "Iteration 19, loss = 0.63550959\n",
      "Validation score: 0.622591\n",
      "Iteration 20, loss = 0.63233312\n",
      "Validation score: 0.623447\n",
      "Iteration 21, loss = 0.63165560\n",
      "Validation score: 0.622935\n",
      "Iteration 22, loss = 0.63133095\n",
      "Validation score: 0.623512\n",
      "Iteration 23, loss = 0.62959219\n",
      "Validation score: 0.621073\n",
      "Iteration 24, loss = 0.62810428\n",
      "Validation score: 0.619891\n",
      "Iteration 25, loss = 0.63255202\n",
      "Validation score: 0.622981\n",
      "Iteration 26, loss = 0.62923240\n",
      "Validation score: 0.623149\n",
      "Iteration 27, loss = 0.62492636\n",
      "Validation score: 0.621641\n",
      "Iteration 28, loss = 0.62832928\n",
      "Validation score: 0.622507\n",
      "Iteration 29, loss = 0.62668180\n",
      "Validation score: 0.624164\n",
      "Iteration 30, loss = 0.62373044\n",
      "Validation score: 0.621232\n",
      "Iteration 31, loss = 0.62742140\n",
      "Validation score: 0.621288\n",
      "Iteration 32, loss = 0.62290524\n",
      "Validation score: 0.623847\n",
      "Iteration 33, loss = 0.62484684\n",
      "Validation score: 0.618142\n",
      "Iteration 34, loss = 0.62531671\n",
      "Validation score: 0.623614\n",
      "Iteration 35, loss = 0.61993920\n",
      "Validation score: 0.624750\n",
      "Iteration 36, loss = 0.62035153\n",
      "Validation score: 0.618551\n",
      "Iteration 37, loss = 0.62970796\n",
      "Validation score: 0.618049\n",
      "Iteration 38, loss = 0.62381799\n",
      "Validation score: 0.617807\n",
      "Iteration 39, loss = 0.62192877\n",
      "Validation score: 0.622032\n",
      "Iteration 40, loss = 0.61850598\n",
      "Validation score: 0.624173\n",
      "Iteration 41, loss = 0.61944189\n",
      "Validation score: 0.624927\n",
      "Iteration 42, loss = 0.61862845\n",
      "Validation score: 0.617071\n",
      "Iteration 43, loss = 0.61994732\n",
      "Validation score: 0.620543\n",
      "Iteration 44, loss = 0.61729228\n",
      "Validation score: 0.615405\n",
      "Iteration 45, loss = 0.62934721\n",
      "Validation score: 0.615592\n",
      "Iteration 46, loss = 0.62316748\n",
      "Validation score: 0.615489\n",
      "Iteration 47, loss = 0.62159183\n",
      "Validation score: 0.611301\n",
      "Iteration 48, loss = 0.62693122\n",
      "Validation score: 0.620208\n",
      "Iteration 49, loss = 0.62476531\n",
      "Validation score: 0.623019\n",
      "Iteration 50, loss = 0.61995253\n",
      "Validation score: 0.613702\n",
      "Iteration 51, loss = 0.62128714\n",
      "Validation score: 0.623214\n",
      "Iteration 52, loss = 0.61932878\n",
      "Validation score: 0.612427\n",
      "Iteration 53, loss = 0.61853632\n",
      "Validation score: 0.623000\n",
      "Iteration 54, loss = 0.61287971\n",
      "Validation score: 0.621492\n",
      "Iteration 55, loss = 0.61101696\n",
      "Validation score: 0.621837\n",
      "Iteration 56, loss = 0.61081715\n",
      "Validation score: 0.618561\n",
      "Iteration 57, loss = 0.61202068\n",
      "Validation score: 0.622144\n",
      "Iteration 58, loss = 0.60955707\n",
      "Validation score: 0.621343\n",
      "Iteration 59, loss = 0.61086460\n",
      "Validation score: 0.618682\n",
      "Iteration 60, loss = 0.60761603\n",
      "Validation score: 0.624443\n",
      "Iteration 61, loss = 0.60598725\n",
      "Validation score: 0.620040\n",
      "Iteration 62, loss = 0.61296227\n",
      "Validation score: 0.603706\n",
      "Iteration 63, loss = inf\n",
      "Validation score: 0.608267\n",
      "Iteration 64, loss = 0.62084738\n",
      "Validation score: 0.619054\n",
      "Iteration 65, loss = 0.61555757\n",
      "Validation score: 0.616271\n",
      "Iteration 66, loss = 0.61377012\n",
      "Validation score: 0.616048\n",
      "Iteration 67, loss = 0.61551760\n",
      "Validation score: 0.614921\n",
      "Iteration 68, loss = 0.61488468\n",
      "Validation score: 0.620841\n",
      "Iteration 69, loss = 0.61322404\n",
      "Validation score: 0.620953\n",
      "Iteration 70, loss = 0.61027512\n",
      "Validation score: 0.617965\n",
      "Iteration 71, loss = 0.60798910\n",
      "Validation score: 0.617183\n",
      "Iteration 72, loss = 0.60811599\n",
      "Validation score: 0.618784\n",
      "Validation score did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TrainedModels/Final.bin']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it\n",
    "mlp.max_iter=300\n",
    "mlp.n_iter_no_change=30\n",
    "mlp.batch_size=100000\n",
    "mlp.verbose = True\n",
    "mlp.fit(df_train, y_train)\n",
    "dump(mlp, final_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.56      0.57      0.57     64798\n",
      "        male       0.62      0.61      0.62     74300\n",
      "\n",
      "    accuracy                           0.59    139098\n",
      "   macro avg       0.59      0.59      0.59    139098\n",
      "weighted avg       0.59      0.59      0.59    139098\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37005 27793]\n",
      " [28665 45635]]\n"
     ]
    }
   ],
   "source": [
    "mlp = load(final_model_filename)\n",
    "predictions = mlp.predict(df_test)\n",
    "print(metrics.classification_report(y_test, predictions))\n",
    "plot_confusion_matrix(targets=y_test, predicted=predictions, labels=np.unique(y_test))\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   34.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.884925020519195, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.002465844721448737,\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune hyperparmeters by sklearn's random search\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "\n",
    "svm = SVC('rbf')\n",
    "\n",
    "# random search trough the hyperparameters\n",
    "clf_search_cv = RandomizedSearchCV(svm,\n",
    "                                    param_distributions,\n",
    "                                    iid=False,\n",
    "                                    n_iter=30,\n",
    "                                    n_jobs=-1,\n",
    "                                    pre_dispatch='2*n_jobs',\n",
    "                                    cv=3,\n",
    "                                    refit=True,\n",
    "                                    verbose=True, \n",
    "                                    random_state=42,\n",
    "                                    return_train_score=False\n",
    "                                   )\n",
    "\n",
    "clf_search_cv.fit(df_train.iloc[:1000],\n",
    "                  y_train[:1000])\n",
    "\n",
    "svm = clf_search_cv.best_estimator_\n",
    "svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielcauchi/miniconda3/envs/nlp/lib/python3.7/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TrainedModels/SVM.bin']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it\n",
    "svm.max_iter=1000\n",
    "svm.verbose = True\n",
    "svm.fit(df_train, y_train)\n",
    "dump(svm, svm_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.47      0.13      0.21     64798\n",
      "        male       0.53      0.87      0.66     74300\n",
      "\n",
      "    accuracy                           0.53    139098\n",
      "   macro avg       0.50      0.50      0.43    139098\n",
      "weighted avg       0.50      0.53      0.45    139098\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8639 56159]\n",
      " [ 9823 64477]]\n"
     ]
    }
   ],
   "source": [
    "svm = load(svm_model_filename)\n",
    "predictions = svm.predict(df_test)\n",
    "print(metrics.classification_report(y_test, predictions))\n",
    "plot_confusion_matrix(targets=y_test, predicted=predictions, labels=np.unique(y_test))\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
