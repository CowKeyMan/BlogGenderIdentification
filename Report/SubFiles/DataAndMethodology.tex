\section{Data and Methodology}

\subsection{The Corpus}
The corpus used is the Blog Authorship Corpus by \cite{2}. This is a corpus containing posts from 19320 blogs, with 9660 of them being male and another 9660 of them being female. In total there are 344773 male posts and 335010 female posts. Other meta-data includes the age range of the bloggers and their star sign. These will not be considered since the text is the only focus of the task. Other observations on the data include that there are some empty posts, which will be pruned later on. Furthermore, some blogs are spam, with posts containing advertisements or non-sense sentences. These will act as noise in the data. The individual posts were trimmed from trailing spaces and new lines were replaced by full stops prior to any other preprocessing.


\subsection{Feature Extraction and Filtering}
The following 'general' features were extracted from the corpus for each post: Word Count, Sentence Count, Average Word Length, Average Sentence Length, Number of Unique Words/Word Count, Number of URLs/Word Count. The number of words ending with the following suffixes were also counted and divided by the Word Count: able, al, ful, ible, ic, ive, less, ly, ous. This list was obtained from \cite{4} The reason they were divided by the Word Count is because the length of a post may affect the frequency of words in general, hence the features are normalized by this value. It is important to note that the words considered in this step do not include any punctuation.

A few vocabularies were created to extract some important words and bi-grams with regards to the posts. Prior to this however, the posts were split into a train and test set. This was done by first sorting the dataset by file name in ascending order, and getting the number of posts by each author. Then the train set was made to contain around 80\% of each gender. This was made so that an author who is in the train set cannot also be in the test set. Precisely, the training set contains the first 7728 male blogs (270106 posts) and the first 7148 female blogs (269911 posts), so that the number of posts in the train set to balance out. The other 20\% of posts were left for testing. Validation sets will be extracted from the test set later on during model building.

The posts were then converted twice, once as POS tags using the NLTK averaged perceptron tagger \cite{nltk} and once as bleached text \cite{5}. In this work's case, the bleaching technique used was to replace consonants with the letter C and vowels with the letter V. Bleaching allows for abstracting the word itself to avoid content (and language) bias, but rather focusing on the form of the word. The hypothesis here is that men and women use different word forms when writing. 

After the conversion process, a vocabulary of bi-grams and their sum of normalized counts is generated on the train set. Bi-grams were chosen over uni-grams to account for ordering and they were chosen over tri-grams because posts tend to be short, hence these might have ended up being too sparse. The sum of normalized counts, which will be abbreviated to SNC from now on, is the sum of the counts of an item within a document, divided by the total items of that type in that document, summed over all the documents. This is used so that the length of the posts does not skew the counts, otherwise lengthier posts would have more weight. So for an item x, the SNC of x (The bi-grams and later on function words, in the case of this study) over all the documents D (the posts, in the case of this study) is: 

\[SNC(x) = \sum_{d \in D} \frac{number\, of\, times\, x\, occurs\, in\, d}{number\, of\, items\, with\, the\, same\, type\, of\, x\, in\, d}\]

2 values for SNC were generated on the train set for each POS bi-gram and bleached bi-gram. The first value is the SNC over the male written documents and the second on the documents written by females. This resulted in 1,692 POS bi-grams and 2,010,167 bleached bi-grams. These were then filtered by removing those with a total SNC (SNC of a bi-gram over the male written documents + SNC of a bi-gram over the female written documents) of 100 so that the probability of the same bi-grams appearing in the test set as well is high, therefore avoiding sparseness in the data. These were then sorted by their entropy and the top 50 with the least entropy (highest information gain) from each group was taken, so that in the end 50 POS bi-grams and 50 bleached bi-grams were obtained as features. This was done to not have too many unnecessary features. The idea of using entropy is similar to that of using TF-IDF in order to analyse how well a particular item distinguishes between two documents, or sets of documents in this case. However, due to the use of SNC, this method also accounts for post length.

The final set of features were function words. The entire set consisted of 277 function words which can be found in the Appendix. These were pruned much the same way as the bi-grams. Those with a total SNC of 50 or more were taken, then the top 50 of those with least entropy were chosen. It is important to note that when using SNCs, the words used were generated by 'word\textunderscore tokenize' from nltk, so punctuation was considered, unlike in the case of the 'general' features.

The final set of selected function words may be found in the Appendix. After the features were decided, feature extraction was done for each document. When it came to the bi-grams and function word counts, these were divided by the total number of bi-grams or words in that document as well, meaning that the SNC was taken, but D was of size 1.

\subsection{Model Creation}
2 models were created using scikit-learn \cite{scikit-learn}. The first is a Logistic Regression model and was used as a baseline. The second is a multi-layer-perceptron with 1 hidden layer of 265 nodes (the total number of features + 100). Prior to training, the hyperparameters of the models were tuned using scikit-learn's random search, which is similar to grid search but does not search trough all combinations. This evaluates the models created using a cross validation technique. Note: this search was made on 10,000 shuffled examples from the train set. After the best hyperparameters are found, the classifier is fit on the entire train set and tested on the test set. The parameters of the final model may be found in the Appendix.