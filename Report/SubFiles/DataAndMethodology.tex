\section{Data and Methodology}

\subsection{The Corpus}
The corpus used is the Blog Authorship Corpus by \cite{2}. This is a corpus containing posts from 19320 blogs, with 9660 of them being male and another 9660 of them being female. In total there are 344773 male posts and 335010 posts. Other meta-data includes the age range of the bloggers and their star sign. These will not be considered. Other observations on the data include that there are some null posts, which will be pruned later on. Furthermore, some blogs are pure spam, with posts containing garbage such as advertisements or non-sense sentences. These will act as noise in the data. The individual posts were trimmed from trailing spaced and new lines were replaced by fullstops.


\subsection{Feature Extraction and Filtering}
The following 'general' features were extracted from the corpus for each post: Word Count, Sentence Count, Average Word Length, Average Sentence Length, Number of Unique Words/Word Count, Number of URLs/Word Count. The number of words ending with the following suffixes were also counted and divided by the Word Count: able, al, ful, ible, ic, ive, less, ly, ous. This list was obtained from \cite{4} The reason they were divided by the Word Count is because the length of a post may affect the frequency of words in general, hence the features are normalized by this value. It is important to note that the words considered in this case do not include any punctuation.

A few vocabularies were created to extract some important words and bi-grams with regards to the posts. Prior to this however, the posts were split into a train and test set. This was done by first sorting the dataset by file name in ascending order, and getting the number of posts by each author. Then the train set was made to contain round 80\% of each gender. This was made so that author who is in the train set can be in the test set. Precisely, the training set contains the first 7728 male blogs (270106 posts) and the first 7148 female blogs (269911 posts), so that the number of posts balance out. The rest was left for testing.

The posts were then converted twice, once as POS tags using the NLTK averaged perceptron tagger and once as bleached text \cite{5}. In this project's case, the bleaching technique used was to replace consonants with the letter C and vowels with the letter V. Bleaching allows for abstracting the word itself to avoid content (and language) bias, but rather focusing on the form of the word. The hypothesis here is that men and women use different word forms when writing. After the conversion process, a vocabulary of bi-grams and their sum of normalized counts is generated on the train set. Bi-grams were chosen over uni-grams to account for ordering and they were chosen over tri-grams because posts tend to be short, hence these might have ended up being too sparse. The sum of normalized counts, which will be abbreviated to SNC from now on, is the sum of the counts of an item within a document, divided by the total items of that type in that document, for all documents. This is used so that the length of the posts does not skew the counts, such that lengthier posts would have bigger weight. So for an item x, the SNC of x over all the documents(the posts, in this case) D is: 

\[SNC(x) = \sum_{d \in D} \frac{number\, of\, times\, x\, occurs\, in\, d}{number\, of\, items\, with\, the\, same\, type\, of\, x\, in\, d}\]

2 values for SNC were generated on the train set for each POS bi-gram and bleached bi-gram. The first value is the SNC over the male written documents and the second on the second on the documents written by females. This resulted in 1,692 POS bi-grams and 2,010,167 bleached bi-grams. These were then filtered by removing those with total SNC (SNC over the male written documents + SNC over the female written documents) of 100 so that the chances of them appearing in the test set as well is high, therefore avoiding sparseness in the data. These were then sorted by their entropy and the top 50  with the least entropy (highest information gain) from each group was taken, so that in the end 50 POS bi-grams and 50 bleached bi-grams were obtained as features. This was done to not have too many unnecessary features.

The final selected POS bigrams were the following: 'VB VBP', '. .', 'NNP NNPS', 'JJ VBP', 'NNP ``', 'NNP TO', 'DT NNP', 'NNP MD', 'NNP )', '. )', 'CD NNP', '. NN', 'NNP NNS', ') PRP', 'NN VBP', 'PRP CC', '( NNP', '`` DT', 'NNP (', 'PRP WRB', 'NNP VBZ', 'POS NNP', '( CD', '`` NNP', 'IN NNP', 'NNS VBZ', 'NNP CD', '. VBN', 'JJ PRP', 'VBG NNP', 'CD )', 'CD ,', 'VBP VBP', 'NNP DT', 'VBN VBN', '. (', 'NNP IN', 'NNP :', 'CD :', ': NNP', 'VBP PRP', 'VBP WP', 'PRP PRP', 'PRP NN', 'VBP WRB', 'PRP .', 'WP NN', 'DT ``', 'PRP VB', 'VBN DT'.

The final selected bleached bi-grams were the following: 'CVV ?', 'VCCCVCC CVCCVC', '-- --', 'VCCCVCC CVCCVCC', '! V', 'VCCCVCC CCCC', 'VCV CVV', 'CVV !', 'CCCC :', '! CV', '! !', '! CVC', '. :', 'CVCCC !', '! )', '! VCC', '? ?', '! VC', 'CV !', '! CVCV', 'CVCV VCCCVCC', 'CC VCCCVCC', 'CVC !', 'CCV VCCCVCC', 'VC VCCCVCC', '? !', 'VC !', 'CVCC !', 'CCVC VCCCVCC', 'VCCCVCC CVCVC', '! VCCCVCC', 'CC CVVCC', 'CCV ``', "CCV '" ',CVCVC !' ',! CCVC' ',CVCV !' ',CVCVVC VC' ',! CVCC' ',VCCCVCC CVCC' ',CVCVVCV V' ',: V' ',: )' ',: ``' ',CVVC !' ',V VC' ',CCV CVCCVCV' ',VCCCVCC CVVC' "CV '," 'CCV CVCVCVC'.

The final set of features were function words. The entire set of considered function words consisted of 277 function words obtained from \url{https://semanticsimilarity.files.wordpress.com/2013/08/jim-oshea-fwlist-277.pdf}. These were pruned much the same way as the bi-grams. Those with a total SNC of 50 or more were taken, then the top 50 of those with least entropy were chosen. It is important to note that when using SNCs, the words used were generated by 'word\textunderscore tokenize' from nltk, so punctuation was considered.

The final selected function words were the following: i, even, under, mine, most, of, shall, do, second, these, by, some, because, did, its, an, often, during, when, as, few, inside, near, too, also, together, between, rather, never, someone, may, their, everything, always, yes, he, which, has, my, so, etc, am, me, sometimes, myself, her, alone, she.

After the features were decided, feature extraction was done for each document. When it came to the bi-grams and function word counts, these were divided by the total number of bigrams or words in that document as well. 

\subsection{Model Creation}
2 models were created using scikit-learn \cite{scikit-learn}. The first is a Logistic Regression model and was used as a baseline. The second is a multi-layer-perceptron with 1 hidden layer of 265 nodes (the total number of features + 100). Prior to training, the hyperparameters of the models were tuned using scikit-learn's random search, which is similar to grid search but does not search trough all combinations. This evaluates the models created using a cross validation technique. Note: this search was made on 10,000 of the train set. After the best hyperparameters are found, the classifier is fit on the entire train set and tested on the test set.